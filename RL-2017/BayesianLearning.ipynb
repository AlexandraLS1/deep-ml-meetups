{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Reinforcement Learning\n",
    "\n",
    "@Shlomo Kashani & Natan Katz.\n",
    "\n",
    "\n",
    "$$F(x) = I_{x}(p,q) = \\frac{\\int_{0}^{x}{t^{p-1}(1-t)^{q-1}dt}}{B(p,q)}\n",
    "\\hspace{.2in} 0 \\le x \\le 1; p, q > 0$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "from collections import namedtuple\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "\n",
    "import warnings\n",
    "from collections import OrderedDict, defaultdict\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from bokeh.io import output_notebook, show, push_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "\n",
    "from ipywidgets import widgets, interact, interactive\n",
    "from IPython.display import display \n",
    "\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "output_notebook()\n",
    "\n",
    "%watermark -a 'Shlomo' -d -t -v -p numpy,pandas,matplotlib,scipy\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivating example and A/B testing\n",
    "\n",
    "Imagine that you are presented with a several different ads, each with a different (but unknown to you) interaction-appeal to people viewing them. \n",
    "\n",
    "How do you discover which ads have the **highest click-through rate**, while minimizing the number of ad slots wasted displaying lower-performing ads? If a new ad appears in your inventory, how do you learn if it’s worth showing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![ab](ab.png)\n",
    "A/B tests, assuming that they’re run properly, are an extremely useful way of tracking which of several options is superior.\n",
    "\n",
    "**Unfortunately they are required to run to completion** in order to take statistically validated action, during which time and effort might be wasted on underperforming options.\n",
    "\n",
    "Refer to this paper: https://conversionxl.com/12-ab-split-testing-mistakes-i-see-businesses-make-all-the-time/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multi-armed bandit problem\n",
    "\n",
    "In the multi-armed bandit problem, we are at a casino (hence ‘bandit’) playing slot machines (hence ‘armed’). \n",
    "\n",
    "Given that **not all slot machines have the same payout**, if we are playing two slots, then we are going to start seeing different results from the two machines. \n",
    "\n",
    "This leads to the **‘explore vs. exploit dilemma,’** where we are forced to decide between exploiting the **higher-payout machine** and exploring the options (at random) in order to collect more data. \n",
    "\n",
    "To any degree that we choose to exploit, we are adapting our behavior to the observed data, and this is one of the general **premises behind reinforcement learning**. \n",
    "\n",
    "Our goal is to maximize our reward and minimize our loss by increasing our certainty that we are making the right decision.\n",
    "\n",
    "There are several strategies one can use to approach the multi-armed bandit problem, including using the Epsilon-Greedy or the UCB algorithm, but this is also where Bayesian inference comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notion of Regret\n",
    "\n",
    "Regret is the difference between your actual payoff and the payoff you would have collected had you played the optimal (best) options at every opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Distributions\n",
    "=========================\n",
    "\n",
    "Let's have a look how different statistical distributions look like, to have a better\n",
    "idea what to use as prior on bayesian exploration.\n",
    "\n",
    "All the distributions available in scipy can be found on the docs here: http://docs.scipy.org/doc/scipy/reference/stats.html#module-scipy.stats\n",
    "\n",
    "Let's start with Discrete distributions\n",
    "\n",
    "Discrete Distributions\n",
    "----------------------\n",
    "\n",
    "* bernoulli:\tA Bernoulli discrete random variable.\n",
    "* binom:\tA binomial discrete random variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from scipy.stats import bernoulli, binom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli distribution\n",
    "-----------------------\n",
    "\n",
    "For an in depth discussion of PDF's/CDF's see: http://work.thaslwanter.at/Stats/html/statsDistributions.html\n",
    "\n",
    "Given a probability $p$, the Bernoulli distribution takes value $k=1$, while having $k=0$ in all the other cases $1-p$.\n",
    "\n",
    "In other words:\n",
    "\n",
    "$$\n",
    "f(k;p) = \\begin{cases}\n",
    "    p & \\text{if } k=1 \\\\\\\\ \n",
    "    1-p & \\text{if } k=0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli.rvs(0.77, size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli PDF Visualization\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(2)\n",
    "\n",
    "colors = plt.rcParams['axes.color_cycle']\n",
    "plt.figure(figsize=(12,8))\n",
    "for i, p in enumerate([0.5, 0.22, 0.66, 0.8, 0.9]):\n",
    "    ax = plt.subplot(1, 5, i+1)\n",
    "    plt.bar(a, bernoulli.pmf(a, p), label=p, color=colors[i], alpha=0.5)\n",
    "    ax.xaxis.set_ticks(a)\n",
    "\n",
    "    plt.legend(loc=0)\n",
    "    if i == 0:\n",
    "        plt.ylabel(\"PDF at $k$\")    \n",
    "\n",
    "plt.suptitle(\"Bernoulli PDF Visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binomial distribution\n",
    "---------------------\n",
    "\n",
    "The Binomial is associated with the question “Out of a given number of trials, how many will succeed?” Some example questions that are modeled with a Binomial distribution are:\n",
    "\n",
    "Out of ten tosses, how many times will this coin land ”heads”?\n",
    "From the children born in a given hospital on a given day, how many of them will be girls?\n",
    "How many students in a given classroom will have green eyes?\n",
    "How many mosquitos, out of a swarm, will die when sprayed with insecticide?\n",
    "We conduct nn repeated experiments where the probability of success is given by the parameter $p$ and add up the number of successes. This number of successes is represented by the random variable $X$. The value of $X$ is then between 0 and $n$.\n",
    "\n",
    "We conduct $n$ repeated experiments where the probability of success is given by the parameter $p$ and add up the number of successes. This number of successes is represented by the random variable $X$. The value of $X$ is then between $0$ and $n$.\n",
    "\n",
    "The binomial distribution is defined as:\n",
    "\n",
    "\\begin{split}P\\left[X = k\\right] = \\begin{cases} {n \\choose k} p^k \\left(1-p\\right)^{n-k}\\ & 0 \\le k \\le n \\\\ 0 & \\mbox{otherwise} \\end{cases} \\quad 0 \\leq p \\leq 1, \\quad n \\in \\mathbb{N}\\end{split}\n",
    "\n",
    "where \n",
    "\n",
    "$${n \\choose k} = \\frac{n!}{k!(n-k)!}$$\n",
    "\n",
    "with $k={1, 2, 3, \\ldots}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binomial PMF Visualization\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "k = np.arange(0, 22)\n",
    "for p, color in zip([0.5, 0.2, 0.66, 0.8, .95], colors):\n",
    "    rv = binom(20, p)\n",
    "    plt.plot(k, rv.pmf(k), lw=2, color=color, label=p)\n",
    "    plt.fill_between(k, rv.pmf(k), color=color, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title(\"Binomial distribution\")\n",
    "plt.tight_layout()\n",
    "plt.ylabel(\"PDF at $k$\")\n",
    "plt.xlabel(\"$k$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Beta distribution (PDF)\n",
    "-----------------\n",
    "**The PDF**, or density of a continuous random variable, is a function that describes the relative likelihood for a random variable XX to take on a given value xx. In the mathematical fields of probability and statistics, a random variate x is a particular outcome of a random variable X: the random variates which are other outcomes of the same random variable might have different values.\n",
    "\n",
    "Since the likelihood to find any given value cannot be less than zero, and since the variable has to have some value, the PDF has the following properties:\n",
    "\n",
    "$$ PDF(x) \\geq 0\\,\\forall \\,x \\in \\mathbb{R} $$\n",
    "\n",
    "$$ \\int\\limits_{ - \\infty }^\\infty  {PDF(x)dx = 1} $$\n",
    "\n",
    "![PDF](PDF.png)\n",
    "*Probability Density Function (PDF) of a value x. The integral over the PDF between a and b gives the likelihood of finding the value of x in that range.*\n",
    "\n",
    "The Beta distribution is defined for a variabile between 0 and 1. \n",
    "\n",
    "The pdf is defined as:\n",
    "\n",
    "$$\n",
    "beta.pdf(x, \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)}x^{\\alpha-1}(1 - x)^{\\beta-1}, \\; with \\;  0≤x≤1, \\alpha>0, \\beta>0\n",
    "$$\n",
    "\n",
    "The Beta distribution is very useful in Bayesian statistics, $B$ above is the [Beta function](http://en.wikipedia.org/wiki/Beta_function) (hence the name). \n",
    "\n",
    "The random variable $X$ is only allowed in [0,1], making the Beta distribution a popular distribution for decimal values, probabilities and proportions. The values of $\\alpha$ and $\\beta$, both positive values, provide great flexibility in the shape of the distribution.\n",
    "\n",
    "We note that the mean/var of the distribution are:\n",
    "\n",
    "$$\\begin{align} \\ E(\\theta) = \\frac{a}{a+b}, \\end{align}$$\n",
    "\n",
    "$$\\begin{align} \\ var(\\theta) = \\frac{ab}{(a+b)^2(a+b+1)} \\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Beta distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = scipy.stats.beta\n",
    "x = np.linspace(0,1, num=200)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "for a, b, c in zip([0.5, 1, 1, 2, 10], [0.5, 1, 3, 2, 5], colors):\n",
    "    plt.plot(x, beta.pdf(x, a, b), lw=2, \n",
    "             c=c, label = r\"$\\alpha = {0:.1f}, \\beta={1:.1f}$\".format(a, b))\n",
    "    plt.fill_between(x, beta.pdf(x, a, b), color=c, alpha = .1)\n",
    "    \n",
    "    \n",
    "plt.legend(loc=0)\n",
    "plt.ylabel(\"PDF at $x$\")\n",
    "plt.xlabel(\"$x$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDF (Cumulative Distribution Function) of Beta\n",
    "\n",
    "The probability to find a value between $a$ and $b$ is given by the integral over the PDF in that range and the Cumulative Distribution Function tells you for each value which percentage of the data has a lower value:\n",
    "\n",
    "$$\\mathbb{P}(a \\leq X \\leq b) = \\int\\limits_a^b {PDF(x)dx} = CDF(b) - CDF(a)$$\n",
    "\n",
    "![CDF-PDF](PDF_CDF.png)\n",
    "*Probability Density Function (left) and Cumulative distribution function (right) of a normal distribution.*\n",
    "\n",
    "The formula for the cumulative distribution function of the beta distribution is also called the incomplete beta function ratio (commonly denoted by $Ix$) and is defined as:\n",
    "\n",
    "$$F(x) = I_{x}(p,q) = \\frac{\\int_{0}^{x}{t^{p-1}(1-t)^{q-1}dt}}{B(p,q)}\n",
    "\\hspace{.2in} 0 \\le x \\le 1; p, q > 0$$ \n",
    "\n",
    "\n",
    "![CDF](betcdf4.gif)\n",
    "\n",
    "Show this:\n",
    "https://www.countbayesie.com/blog/2015/4/4/parameter-estimation-the-pdf-cdf-and-quantile-function\n",
    "\n",
    "\n",
    "References:\n",
    "http://www.boost.org/doc/libs/1_35_0/libs/math/doc/sf_and_dist/html/math_toolkit/special/sf_beta/ibeta_function.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Properties of the Beta Distribution\n",
    "\n",
    "## Connection with Uniform\n",
    "A Beta, specified by parameters $(1,1)$ is **the Uniform distribution**. Hence the Beta distribution is a generalization of the Uniform distribution.\n",
    "\n",
    "## Conjugate prior\n",
    "The second is that there is an interesting connection between the Beta distribution and the Binomial distribution. Suppose we are interested in some unknown proportion or probability $p$. We assign a $\\text{Beta}(\\alpha, \\beta)$ prior to $p$. We observe some data generated by a Binomial process, say $X \\sim \\text{Binomial}(N, p)$, with $p$ still unknown. Then our posterior *is again a Beta distribution*, i.e. $p | X \\sim \\text{Beta}( \\alpha + X, \\beta + N -X )$. Succinctly, one can relate the two by \"a Beta prior with Binomial observations creates a Beta posterior\".\n",
    "\n",
    "In light of the above two paragraphs, if we start with a $\\text{Beta}(1,1)$ prior on $p$ (which is a Uniform), observe data $X \\sim \\text{Binomial}(N, p)$, then our posterior is $\\text{Beta}(1 + X, 1 + N - X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Beta-Binom Notebook\n",
    "\n",
    "https://github.com/QuantScientist/deep-ml-meetups/blob/master/bayesian-deep-learning/python/BetaBinSymbolically.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations: \n",
    "The mean of each distribution gradually converges around its true value. \n",
    "\n",
    "But we can see that the highest-payout bandit has the lowest variance, which is a reflection of the fact that it has the highest N. \n",
    "\n",
    "However, this is not necessarily a problem, because the by the end there is almost no overlap between the highest distribution and the two lower ones, meaning that the probability that sampling from the inferior bandits would yield a higher payout is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we want to solve is the following. \n",
    "\n",
    "You have come up with $K$ different variations of a webpage (e.g. different layout) that now you wish to find the ones with the best click through rate (CTR), e.g. clicking to sign-up for the newsletter. \n",
    "\n",
    "Let's represent each CTR by $\\theta_i$ - i.e., $\\theta_i$ is the true probability that an individual user will click when they were shown with the $i_{th}$ webpage. \n",
    "\n",
    "It is important to note that we **don't actually know** what $\\theta_i$ is - if we did, we could simply choose ii for which $\\theta_i$ was largest and move on. We're simply pretending that we know in order to simulate the performance of the algorithm.\n",
    "\n",
    "Using the Bayesian approach we will construct a prior probability distribution which represents our original belief about what the actual value of $\\theta_i$, our ctr for the $i_{th}$ webpage is. The prior we'll use is the Beta distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Bandits Flow\n",
    "\n",
    "For each machine, it constructs a beta distribution as a prior. By multiplying it with the likelihood of getting a success, it constructs the posterior distribution for each machine. \n",
    "\n",
    "This **posterior** corresponds to our belief at a time point on what the true probability of success is, if we draw from that machine. \n",
    "\n",
    "In each step, it samples from each of the posterior distributions. \n",
    "\n",
    "It then chooses the sample with the **highest probability**. \n",
    "\n",
    "If one machine stands out, clearly, it will be the maximum in most cases, when samples are drawn, and correspond to the exploit phase. Whereas, if two machine have similar posterior distribution, this means, to a Bayesian, from our knowledge, we aren't very sure, who is better. So, in a sense then, the maximum is more likely to come from either distribution. \n",
    "\n",
    "In plain English, this is how Thompson sampling solves the explore exploit dilemma mentioned above.\n",
    "\n",
    "Here's a quick recap of the distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Does Thompson Sampling Strategy Work?\n",
    "\n",
    "Thompson sampling works by maintaining a prior on the the mean rewards of the arms $\\mu_i$. It samples values for each arm from its prior and picks the arm with the highest value. \n",
    "\n",
    "When an arm $a$ is pulled and a Bernoulli reward $r$ is observed, it modifies the prior based on the reward. This procedure is repeated for the next arm pull. \n",
    "\n",
    "Beta distribution is a convenient choice of priors for bernoulli rewards. The probability density function of a Beta distribution with parameters $\\alpha$ and $\\beta$ is:\n",
    "\n",
    "$$\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to regret: Expected cumulative regret\n",
    "\n",
    "The expected cumulative regret for Thompson sampling strategy is **logarithmic**. \n",
    "\n",
    "Though this strategy has been around since the 1930s, its regert bound has been proved only recently in this paper: [Analysis of Thompson Sampling for the Multi-armed Bandit Problem](http://jmlr.org/proceedings/papers/v23/agrawal12/agrawal12.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to show a demo\n",
    "----------------------\n",
    "\n",
    "https://learnforeverlearn.com/bandits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ipython nbconvert BayesianLearning.ipynb --to slides --post serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://zlatankr.github.io/posts/2017/04/07/bayesian-ab-testing\n",
    "# http://souravc83.github.io/tale-of-two-bandits-bayesian-and-greedy.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Notes: Algorithms for the multi-armed bandit problem](http://www.cs.mcgill.ca/~vkules/bandits.pdf)\n",
    "- [Blog: Bandits for Recommendation Systems](http://engineering.richrelevance.com/bandits-recommendation-systems/)\n",
    "- [Blog: When to Run Bandit Tests Instead of A/B Tests](http://conversionxl.com/bandit-tests/)\n",
    "- [Blog: Bayesian Bandits - optimizing click throughs with statistics](https://www.chrisstucchio.com/blog/2013/bayesian_bandit.html)\n",
    "- [Blog: Balancing Earning with Learning: Bandits and Adaptive Optimization](http://conductrics.com/balancing-earning-with-learning-bandits-and-adaptive-optimization/)\n",
    "- [Notebook: Bayesian Methods for Hackers Chapter 6](http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter6_Priorities/Ch6_Priors_PyMC2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
